# Gemini-CLI ZAI Fork

Fork of gemini-cli adapted to use ZAI's GLM-4.7 API for benchmarking.

## Objectives

1. Implement ZAI with thinking capabilities (GLM-4.7)
2. Add mode to disable thinking
3. Connect web search to ZAI's API

---

## ZAI API Reference (Verified by Testing)

### Endpoints

| Endpoint                                               | Purpose                             | Notes                                  |
| ------------------------------------------------------ | ----------------------------------- | -------------------------------------- |
| `https://api.z.ai/api/coding/paas/v4/chat/completions` | **Coding Plan** (used in this fork) | Preserved Thinking enabled by default  |
| `https://api.z.ai/api/paas/v4/chat/completions`        | Standard API                        | Preserved Thinking disabled by default |

### Authentication

```
Authorization: Bearer $ZAI_API_KEY
```

### Thinking Configuration

```json
{
  "thinking": {
    "type": "enabled", // "enabled" | "disabled"
    "clear_thinking": false // false = Preserved Thinking (keep reasoning in context)
  }
}
```

**Key Points:**

- `type: "enabled"` (default for GLM-4.7): Model reasons before answering,
  returns `reasoning_content`
- `type: "disabled"`: Direct answers, no reasoning, faster/cheaper (~2 tokens vs
  ~70+ for same question)
- `clear_thinking: false`: Preserve reasoning across turns (better for
  multi-turn coding sessions)
- `clear_thinking: true`: Clear reasoning each turn (saves tokens but loses
  context)

**Turn-level Thinking** (GLM-4.7): You can toggle `thinking.type` on each
request within the same session:

- Enable for complex planning, debugging, multi-constraint reasoning
- Disable for quick tool execution, simple facts, formatting requests

**Critical Preserved Thinking Constraint:**

> All consecutive `reasoning_content` blocks must **exactly match the original
> sequence** generated by the model. Do not reorder or edit these blocks;
> otherwise, performance degrades and cache hit rates drop.

### Response Structure

```json
{
  "choices": [{
    "message": {
      "content": "Final answer",
      "reasoning_content": "Thinking process...",  // Only if thinking enabled
      "role": "assistant",
      "tool_calls": [...]  // If tools requested
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 17,
    "completion_tokens": 72,
    "total_tokens": 89,
    "prompt_tokens_details": {"cached_tokens": 2},         // Preserved Thinking cache hits
    "completion_tokens_details": {"reasoning_tokens": 69}  // Tokens spent on thinking
  }
}
```

### Streaming (SSE)

Request with `"stream": true`. Response format:

```
data: {"choices":[{"delta":{"reasoning_content":"..."}}]}
data: {"choices":[{"delta":{"content":"..."}}]}
data: {"choices":[{"finish_reason":"stop"}],"usage":{...}}
data: [DONE]
```

### Tool Calling

Standard OpenAI-compatible format:

```json
{
  "tools": [{
    "type": "function",
    "function": {
      "name": "tool_name",
      "description": "...",
      "parameters": {"type": "object", "properties": {...}}
    }
  }],
  "tool_choice": "auto"  // "auto" | "none" | "required" | {"type":"function","function":{"name":"..."}}
}
```

**Interleaved Thinking Flow** (tool calls with reasoning):

```
1. User message
2. Assistant reasons (reasoning_content) → decides to call tool
3. Tool executes, returns result
4. Assistant reasons about result (reasoning_content) → final answer or more tools
```

**Preserving reasoning across tool calls** - Include full `reasoning_content` in
message history:

```json
// After assistant response with tool call:
{"role": "assistant", "content": "", "reasoning_content": "I need to check...", "tool_calls": [{"id": "call_1", "type": "function", "function": {"name": "get_weather", "arguments": "{\"city\":\"Paris\"}"}}]}

// After tool result:
{"role": "tool", "tool_call_id": "call_1", "content": "{\"temp\": \"22°C\"}"}

// Next request - model continues reasoning from preserved context
```

### Web Search

**Option 1: Web Search in Chat** (LLM processes search results)

```json
{
  "tools": [
    {
      "type": "web_search",
      "web_search": {
        "enable": "True",
        "search_engine": "search-prime",
        "count": "5",
        "search_domain_filter": "example.com", // Optional: restrict to domain
        "search_recency_filter": "noLimit", // Optional: "day", "week", "month", "year", "noLimit"
        "search_result": "True", // Return raw search results in response
        "search_prompt": "Summarize key points from {{search_result}}", // Optional: custom processing
        "content_size": "high" // Summary length: "low", "medium", "high"
      }
    }
  ]
}
```

Response includes both LLM answer and raw search results:

```json
{
  "choices": [{ "message": { "content": "Based on search results..." } }],
  "web_search": [
    {
      "title": "...",
      "link": "https://...",
      "content": "...",
      "media": "Site Name",
      "refer": "ref_1"
    }
  ]
}
```

**Option 2: Standalone Web Search API** (raw results only, no LLM)

```
POST https://api.z.ai/api/paas/v4/web_search
{
  "search_engine": "search-prime",
  "search_query": "your query",
  "count": 10
}
```

> Note: Requires separate billing. Coding endpoint includes web search in chat
> for free.

**Option 3: MCP Server** (for Cursor, Claude Code, etc.)

```json
{
  "mcpServers": {
    "z.ai-web-search": {
      "url": "https://api.z.ai/api/mcp/web_search/sse?Authorization=YOUR_API_KEY"
    }
  }
}
```

---

## Implementation Notes

### Current Implementation (`packages/core/src/core/glmContentGenerator.ts`)

- Uses coding endpoint by default
- Thinking always enabled with `clear_thinking` configurable
- Supports streaming with proper SSE parsing
- Maps Gemini tool format to OpenAI-compatible format
- Handles `reasoning_content` → `thought: true` part conversion

### Key Considerations

1. **Token Efficiency**: Disable thinking for simple queries (facts, formatting)
2. **Preserved Thinking**: Keep `clear_thinking: false` for multi-turn coding
   sessions
3. **Tool Calls**: Always return `reasoning_content` with tool results to
   maintain reasoning continuity
4. **Cache Hits**: `cached_tokens` in response indicates Preserved Thinking is
   working

### Models

- `glm-4.7` - Latest with full thinking support
- `glm-4.6` - Previous generation
- `glm-4-air` - Faster/cheaper variant
